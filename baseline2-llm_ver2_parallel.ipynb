{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1008fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, LlamaForSequenceClassification, LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac544e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['WANDB_PROJECT'] = 'kaggle-llm-2023-llama-2-7b-test-train'\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c6561f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_dir = '/root/autodl-tmp/llama2'\n",
    "tokenizer_dir = '/root/autodl-tmp/llama2'\n",
    "model_dir = '/root/autodl-tmp/llama2'\n",
    "config = LlamaConfig.from_pretrained(config_dir)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "## set up the pad token\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "tokenizer.padding_side = \"left\"\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#set up the numnber of labels\n",
    "config.num_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c83471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b251e95a01634422986faa98da44a6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(model_dir, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f131b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer, config):\n",
    "    sep = tokenizer.sep_token\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 256\n",
    "    )\n",
    "    return {\n",
    "        ** tokenized\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b6a4b4",
   "metadata": {},
   "source": [
    "## Read in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95053ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44868, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('/root/autodl-tmp/llm-2023/test_essays.csv')\n",
    "sub = pd.read_csv('/root/autodl-tmp/llm-2023/sample_submission.csv')\n",
    "org_train = pd.read_csv('/root/autodl-tmp/llm-2023/train_essays.csv')\n",
    "\n",
    "train = pd.read_csv('/root/autodl-tmp/llm-2023/train_v2_drcat_02.csv', sep = ',')\n",
    "display(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba4c6c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44868, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_gen_data = pd.read_parquet('/root/autodl-tmp/llm-2023/gen_data_21122023.parquet')\n",
    "display(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619eadd6-e64e-434c-8c59-1bd0a97e4279",
   "metadata": {},
   "source": [
    "## Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee127214-6549-45ae-8094-2fdc9dc305dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f26d94",
   "metadata": {},
   "source": [
    "## The util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4278e53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_train(df, tokenizer):\n",
    "    tokenized = tokenizer(\n",
    "        df['text'],\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 256\n",
    "    )\n",
    "    \n",
    "    labels = df['label']\n",
    "    return {\n",
    "        **tokenized,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea439dc6",
   "metadata": {},
   "source": [
    "## Split the training data using groupkfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f8d45e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "group_kfold = GroupKFold(n_splits= 5)\n",
    "groups = train.prompt_name\n",
    "group_kfold.get_n_splits(train['text'], train['label'], groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1535906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer = tokenizer,\n",
    "    pad_to_multiple_of=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9afc9c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config.update({\n",
    "    'hidden_dropout_prob': 0,\n",
    "    'attention_probs_dropout_prob': 0,\n",
    "    'num_labels': 2,\n",
    "    'problem_type': 'single_label_classification',\n",
    "    'max_position_embeddings': 256,\n",
    "    'num_proc': 4\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402670a9",
   "metadata": {},
   "source": [
    "## Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9091f0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def compute_auc(eval_pred):\n",
    "    preds, labels = eval_pred;\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, preds, pos_label = 1)\n",
    "    auc = metrics.auc(fpr, tpr);\n",
    "    return {\n",
    "        'auc': auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08c363-9923-4da3-b623-ca75708dfa69",
   "metadata": {},
   "source": [
    "## Define the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c6f24eb-36c1-4310-95bb-cd4d3c2b8e00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df;\n",
    "        self.tokenizer = tokenizer;\n",
    "        self.max_len = config.max_position_embeddings\n",
    "    def __len__(self):\n",
    "        return len(self.df);\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_row = self.df.iloc[index, :]\n",
    "        labels = data_row['label']\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            data_row['text'],\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            max_length = self.max_len\n",
    "        )\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(ids, dtype = torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype = torch.long),\n",
    "            'labels': labels\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a329063-eef4-4269-b152-197ae126b36c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  \n",
       "0  Phones and driving  persuade_corpus          False  \n",
       "1  Phones and driving  persuade_corpus          False  \n",
       "2  Phones and driving  persuade_corpus          False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b2127-a669-488c-9027-7722e0a2fd20",
   "metadata": {},
   "source": [
    "## Test training function coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c3a1e4f-e1a0-425b-953c-c124e1a623eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_train = train.iloc[:100, :].reset_index(drop= True)\n",
    "test_val = train.iloc[100:200, :].reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7e428bd-996b-4746-9b2e-a88d48010ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TrainDataset(test_train, tokenizer)\n",
    "valid_dataset = TrainDataset(test_val, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = 1,\n",
    "    shuffle = True,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size = 1,\n",
    "    shuffle= False,\n",
    "    num_workers = 4,\n",
    "    pin_memory = True,\n",
    "    drop_last = True\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91e74bfd-721b-4e8a-a78e-37496d61d2df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_params(model, lr, weight_decay = 0.0):\n",
    "    param_optimizer = list(model.named_parameters());\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'lr': lr, 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "         'lr': lr, 'weight_decay': 0.0},\n",
    "    ]\n",
    "    \n",
    "    return optimizer_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca84fe7b-12f3-4510-a59d-1d677a6d1653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = model.to(device)\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87310df1-8dfe-45bd-8109-9af4e2c1aadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "optimizer_params = get_optimizer_params(model, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f5737cf-d758-4aef-938f-9f1e44740db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_loader):\n",
    "    for key,value in batch.items():\n",
    "        batch[key] = value.to('cuda:1')\n",
    "    batch_size = batch['labels'].size(0)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c4352cf-7d9c-453c-a4dc-80e3a2c2528a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "049a820a-8600-434d-9ab5-8f0da9256d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = model(**batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3a4f5-e164-477b-bb25-88c556208f30",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4f5e3-2d7a-4d67-8f9f-f93c17492cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device,\n",
    "             valid_loader, start_time, best_sore):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        for key, batch in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "    loss = model(**batch).loss\n",
    "    losses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c216cf73-d54f-43de-a48b-09632ba10831",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test train-val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25cd1452-3706-4145-8e06-ae7ce444ceb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, disable_progress_bar\n",
    "train_ds = Dataset.from_pandas(train.iloc[:30000, :])\n",
    "val_ds = Dataset.from_pandas(train.iloc[30000:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe0ddfd6-c96b-4d15-a7bf-bb8b3c8e9b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248d8cde604f4f8d95393550efe8e708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab2c4a023ba4baf90f632ed9710da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/14868 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = train_ds.map(\n",
    "    tokenize_train,\n",
    "    batched = False,\n",
    "    num_proc = config.num_proc,\n",
    "    fn_kwargs = {'tokenizer': tokenizer}\n",
    ")\n",
    "\n",
    "val_ds = val_ds.map(\n",
    "    tokenize_train,\n",
    "    batched = False,\n",
    "    num_proc = config.num_proc,\n",
    "    fn_kwargs = {'tokenizer': tokenizer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e086c928-4137-4737-8502-c9018ed00db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer = tokenizer,\n",
    "    pad_to_multiple_of=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "496705c8-b704-4920-93c6-2b65574edb28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=6e-6,\n",
    "    weight_decay=1e-4,\n",
    "    warmup_ratio=0,\n",
    "    lr_scheduler_type='cosine',\n",
    "    optim='adamw_torch',\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=150,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=150,\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=1,\n",
    "    save_steps=150,\n",
    "    report_to='wandb',\n",
    "    run_name='test-llm-2023-llama2-run',\n",
    "    metric_for_best_model='auc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52aab4b4-201d-41d7-8ea0-0b7fa1d17572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICE'] = \"0,1\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = val_ds,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_auc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a229445a-fb25-426a-8ef7-f80f9189bd39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_sun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731520a1367448a1bcb11034cc7a803e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112955336769422, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/wandb/run-20240107_230716-ab0v5mil</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peng_sun/kaggle-llm-2023-llama-2-7b-test-train/runs/ab0v5mil' target=\"_blank\">test-llm-2023-llama2-run</a></strong> to <a href='https://wandb.ai/peng_sun/kaggle-llm-2023-llama-2-7b-test-train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peng_sun/kaggle-llm-2023-llama-2-7b-test-train' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-llm-2023-llama-2-7b-test-train</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peng_sun/kaggle-llm-2023-llama-2-7b-test-train/runs/ab0v5mil' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-llm-2023-llama-2-7b-test-train/runs/ab0v5mil</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 1346, in forward\n    transformer_outputs = self.model(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 1068, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 796, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 413, in forward\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 31.74 GiB total capacity; 30.86 GiB already allocated; 363.12 MiB free; 30.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:2758\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2757\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    170\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:181\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:89\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     87\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m---> 89\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py\", line 64, in _worker\n    output = module(*input, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 1346, in forward\n    transformer_outputs = self.model(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 1068, in forward\n    layer_outputs = decoder_layer(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 796, in forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/root/miniconda3/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py\", line 413, in forward\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 442.00 MiB (GPU 0; 31.74 GiB total capacity; 30.86 GiB already allocated; 363.12 MiB free; 30.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49df0ef7-a554-4902-97d8-4a9fb193e4ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d757433-98d0-4ce4-b23a-6675b419a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
